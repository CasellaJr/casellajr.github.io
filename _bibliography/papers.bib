---
---

@inproceedings{24:casella:fedrec,
  title = {Federated Learning in a Semi-Supervised Environment for Earth Observation Data},
  author = {Bruno Casella and Alessio Barbaro Chisari and Marco Aldinucci and Sebastiano Battiato and Mario Valerio Giuffrida},
  url = {https://iris.unito.it/retrieve/a798d7b8-6b98-48c2-92f4-327d2aaa8788/ES2024-214.pdf},
  doi = {10.14428/esann/2024.es2024-214},
  year = {2024},
  date = {2024-10-01},
  booktitle = {Proceedings of the 32nd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN},
  address = {Bruges, Belgium},
  abstract = {We propose FedRec, a federated learning workflow taking advantage of unlabelled data in a semi-supervised environment to assist in the training of a supervised aggregated model. In our proposed method, an encoder architecture extracting features from unlabelled data is aggregated with the feature extractor of a classification model via weight averaging. The fully connected layers of the supervised models are also averaged in a federated fashion. We show the effectiveness of our approach by comparing it with the state-of-the-art federated algorithm, an isolated and a centralised baseline, on novel cloud detection datasets.},
  keywords = {ai, epi, icsc},
  pubstate = {published},
  tppubtype = {inproceedings},
  selected           = {true},
  abbr = {ESANN}
}

@inproceedings{24:casella:frocks,
  title = {Federated Time Series Classification with ROCKET features},
  author = {Bruno Casella and Jakobs Matthias and Marco Aldinucci and Sebastian Buschjager},
  url = {https://iris.unito.it/retrieve/51b63fc1-3e22-4ad4-8926-84af69cde739/ES2024-61.pdf},
  doi = {10.14428/esann/2024.es2024-61},
  year = {2024},
  date = {2024-10-01},
  booktitle = {Proceedings of the 32nd European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, ESANN},
  address = {Bruges, Belgium},
  abstract = {This paper proposes FROCKS, a federated time series classification method using ROCKET features. Our approach dynamically adapts the models’ features by selecting and exchanging the best-performing ROCKET kernels from a federation of clients. Specifically, the server gathers the best-performing kernels of the clients together with the associated model parameters, and it performs a weighted average if a kernel is best-performing for more than one client. We compare the proposed method with state-of-the-art approaches on the UCR archive binary classification datasets and show superior performance on most datasets.},
  keywords = {ai, epi, icsc},
  pubstate = {published},
  tppubtype = {inproceedings},
  selected           = {true},
  abbr = {ESANN}
}

@inproceedings{24:casella:sgx,
  title = {A Performance Analysis for Confidential Federated Learning},
  author = {Bruno Casella and Iacopo Colonnelli and Gianluca Mittone and Robert Birke and Walter Riviera and Antonio Sciarappa and Carlo Cavazzoni and Marco Aldinucci},
  url = {https://iris.unito.it/retrieve/b5877a97-2d8d-4e95-8791-0aa4a1b953b3/DLSP___CONFIDENTIAL_FL.pdf},
  doi = {10.1109/SPW63631.2024.00009},
  year = {2024},
  date = {2024-05-01},
  booktitle = {Proceedings of the 2024 Deep Learning Security and Privacy Workshop, IEEE Symposium on Security and Privacy 2024},
  address = {San Francisco, CA},
  abstract = {Federated Learning (FL) has emerged as a solution to preserve data privacy by keeping the data locally on each participant's device. However, FL alone is still vulnerable to attacks that can cause privacy leaks. Therefore, it becomes necessary to take additional security measures at the cost of increasing runtimes. The Trusted Execution Environment (TEE) approach promises to offer the highest degree of security during execution. However, TEEs suffer from memory limits which prevent safe end-to-end FL training of modern deep models. State-of- the-art approaches limit secure training to selected layers, failing to avert the full spectrum of attacks or adopt layer-wise training affecting model performance. We benchmark the usage of a library OS (LibOS) to run the full, unmodified end-to-end FL training inside the TEE. We extensively evaluate and model the overhead of the different security mechanisms needed to protect the data and model during computation (TEE), communication (TLS), and storage (disk encryption). The obtained results across three datasets and two models demonstrate that LibOSes are a viable way to seamlessly inject security into FL with limited overhead (at most 2x), offering valuable guidance for researchers and developers aiming to apply FL in data-security-focused contexts.},
  keywords = {ai, confidential, epi, icsc},
  pubstate = {published},
  tppubtype = {inproceedings},
  abbr = {SP}
}

@inproceedings{harrak2024fedsurvboost,
  title = {Federated AdaBoost for Survival Analysis},
  author = {Oussama Harrak and Bruno Casella and Samuele Fonio and Piero Fariselli and Gianluca Mittone and Cesare Rollo and Tiziana Sanavia and Marco Aldinucci},
  year = {2024},
  date = {2024-01-01},
  booktitle = {Proceedings of the ECML-PKDD Workshop, 2nd workshop on advancements in Federated Learning},
  address = {Vilnius, Lithuania},
  abstract = {This work proposes FedSurvBoost, a federated learning pipeline for survival analysis based on the AdaBoost.F algorithm, which iteratively aggregates the best local weak hypotheses. Our method extends AdaBoost.F by removing the dependence on the number of classes coefficient from the computation of the weights of the best model. This makes it suitable for regression tasks, such as survival analysis. We show the effectiveness of our approach by comparing it with state-of-the-art methods, specifically developed for survival analysis problems, on two common survival datasets. Our code is available at https://github.com/oussamaHarrak/FedSurvBoost.},
  keywords = {epi, icsc},
  pubstate = {published},
  tppubtype = {inproceedings},
  abbr = {ECML}
}

@article{24:casella:normalization,
  title = {Experimenting With Normalization Layers in Federated Learning on Non-IID Scenarios},
  author = {Bruno Casella and Roberto Esposito and Antonio Sciarappa and Carlo Cavazzoni and Marco Aldinucci},
  doi = {10.1109/ACCESS.2024.3383783},
  year = {2024},
  date = {2024-01-01},
  journal = {IEEE Access},
  volume = {12},
  pages = {47961-47971},
  keywords = {epi, icsc},
  pubstate = {published},
  tppubtype = {article},
  abbr = {ACCESS}
}

@article{24:casella:starprotocol,
  title = {Protocol for training MERGE: A federated multi-input neural network for COVID-19 prognosis},
  author = {Bruno Casella and Walter Riviera and Marco Aldinucci and Gloria Menegaz},
  url = {https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3225.pdf},
  doi = {10.1016/j.xpro.2023.102812},
  year = {2024},
  date = {2024-01-01},
  journal = {STAR Protocols},
  institution = {Computer Science Department, University of Torino},
  abstract = {Federated learning is a cooperative learning approach that has emerged as an effective way to address privacy concerns. Here, we present a protocol for training MERGE: a federated multi-input neural network (NN) for COVID-19 prognosis. We describe steps for collecting and preprocessing datasets. We then detail the process of training a multi-input NN. This protocol can be adapted for use with datasets containing both image- and table-based input sources.},
  note = {https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3225.pdf},
  keywords = {epi, icsc},
  pubstate = {published},
  tppubtype = {article},
  abbr = {STAR}
}

@article{23:casella:FedER,
  title = {FedER: Federated Learning through Experience Replay and Privacy-Preserving Data Synthesis},
  author = {Matteo Pennisi and Federica Proietto Salanitri and Giovanni Bellitto and Bruno Casella and Marco Aldinucci and Simone Palazzo and Concetto Spampinato},
  url = {https://www.sciencedirect.com/science/article/pii/S107731422300262X?via%3Dihub},
  doi = {10.1016/j.cviu.2023.103882},
  year = {2024},
  date = {2024-01-01},
  journal = {Computer Vision and Image Understanding},
  volume = {238},
  pages = {103882},
  institution = {Computer Science Department, University of Torino},
  abstract = {In the medical field, multi-center collaborations are often sought to yield more generalizable findings by leveraging the heterogeneity of patient and clinical data. However, recent privacy regulations hinder the possibility to share data, and consequently, to come up with machine learning-based solutions that support diagnosis and prognosis. Federated learning (FL) aims at sidestepping this limitation by bringing AI-based solutions to data owners and only sharing local AI models, or parts thereof, that need then to be aggregated. However, most of the existing federated learning solutions are still at their infancy and show several shortcomings, from the lack of a reliable and effective aggregation scheme able to retain the knowledge learned locally to weak privacy preservation as real data may be reconstructed from model updates. Furthermore, the majority of these approaches, especially those dealing with medical data, relies on a centralized distributed learning strategy that poses robustness, scalability and trust issues. In this paper we present a federated and decentralized learning strategy, FedER, that, exploiting experience replay and generative adversarial concepts, effectively integrates features from local nodes, providing models able to generalize across multiple datasets while maintaining privacy. FedER is tested on two tasks — tuberculosis and melanoma classification — using multiple datasets in order to simulate realistic non-i.i.d. medical data scenarios. Results show that our approach achieves performance comparable to standard (non-federated) learning and significantly outperforms state-of-the-art federated methods in their centralized (thus, more favourable) formulation. Code is available at https://github.com/perceivelab/FedER},
  keywords = {ai},
  pubstate = {published},
  tppubtype = {article},
  selected = {true},
  abbr = {CVIU}
}

@inproceedings{23:casella:onchain,
  title = {Predicting Cryptocurrencies Market Phases through On-Chain Data Long-Term Forecasting},
  author = {Bruno Casella and Lorenzo Paletto},
  url = {https://iris.unito.it/bitstream/2318/1902652/1/6.%20ICBC23%20-%20PREDICTING%20BTC.pdf},
  doi = {https://doi.org/10.1109/ICBC56567.2023.10174989},
  year = {2023},
  date = {2023-01-01},
  booktitle = {Proceedings of the 2023 IEEE International Conference on Blockchain and Cryptocurrency (ICBC), 1-5 May 2023, Dubai},
  abstract = {Blockchain, the underlying technology of Bitcoin and several other cryptocurrencies, like Ethereum, produces a massive amount of open-access data that can be analyzed, providing important information about the network's activity and its respective token. The on-chain data have extensively been used as input to Machine Learning algorithms for predicting cryptocurrencies' future prices; however, there is a lack of study in predicting the future behaviour of on-chain data. This study aims to show how on-chain data can be used to detect cryptocurrency market regimes, like minimum and maximum, bear and bull market phases, and how forecasting these data can provide an optimal asset allocation for long-term investors.},
  note = {https://ieeexplore.ieee.org/document/10174989},
  keywords = {epi, icsc},
  pubstate = {published},
  tppubtype = {inproceedings},
  abbr = {ICBC}
}

@inproceedings{23:casella:architecturalfedavg,
  title = {Architecture-Based FedAvg for Vertical Federated Learning},
  author = {Bruno Casella and Samuele Fonio},
  url = {https://iris.unito.it/retrieve/173d9960-8531-419d-9bd5-5acce6694c4e/Aggregation%20Based%20VFL.pdf},
  doi = {10.1145/3603166.3632559},
  year = {2023},
  date = {2023-01-01},
  booktitle = {Proceedings of the 3rd Workshop on Distributed Machine Learning for the Intelligent Computing Continuum (DML-ICC), IEEE/ACM UCC 2023, Taormina, Italy, 4 December 2023},
  abstract = {Federated Learning (FL) has emerged as a promising solution to address privacy concerns by collaboratively training Deep Learning (DL) models across distributed parties. This work proposes an architecture-based aggregation strategy in Vertical FL, where parties hold data with different attributes but shared instances. Our approach leverages the identical architectural parts, i.e. neural network layers, of different models to selectively aggregate weights, which is particularly relevant when collaborating with institutions holding different types of datasets, i.e., image, text, or tabular datasets. In a scenario where two entities train DL models, such as a Convolutional Neural Network (CNN) and a Multi-Layer Perceptron (MLP), our strategy computes the average only for architecturally identical segments. This preserves data-specific features learned from demographic and clinical data. We tested our approach on two clinical datasets, i.e., the COVID-CXR dataset and the ADNI study. Results show that our method achieves comparable results with the centralized scenario, in which all the data are collected in a single data lake, and benefits from FL generalizability. In particular, compared to the non-federated models, our proposed proof-of-concept model exhibits a slight performance loss on the COVID-CXR dataset (less than 8%), but outperforms ADNI models by up to 12%. Moreover, communication costs between training rounds are minimized by exchanging only the dense layer parameters.},
  note = {https://iris.unito.it/bitstream/2318/1949730/1/HALF_HVL_for_DML_ICC23___Taormina-2.pdf},
  keywords = {ai, epi, icsc},
  pubstate = {published},
  tppubtype = {inproceedings},
  abbr = {DML-ICC}
}

@inproceedings{23:casella:ERGANs,
  title = {Experience Replay as an Effective Strategy for Optimizing Decentralized Federated Learning},
  author = {Matteo Pennisi and Federica Proietto Salanitri and Giovanni Bellitto and Bruno Casella and Marco Aldinucci and Simone Palazzo and Concetto Spampinato},
  url = {https://openaccess.thecvf.com/content/ICCV2023W/VCL/papers/Pennisi_Experience_Replay_as_an_Effective_Strategy_for_Optimizing_Decentralized_Federated_ICCVW_2023_paper.pdf},
  doi = {10.1109/ICCVW60793.2023.00362},
  year = {2023},
  date = {2023-01-01},
  booktitle = {Proceedings of the 1st Workshop on Visual Continual Learning, ICCV 2023, Paris, France, 2 October 2023},
  abstract = {Federated and continual learning are training paradigms addressing data distribution shift in space and time. More specifically, federated learning tackles non-i.i.d data in space as information is distributed in multiple nodes, while continual learning faces with temporal aspect of training as it deals with continuous streams of data. Distribution shifts over space and time is what it happens in real federated learning scenarios that show multiple challenges. First, the federated model needs to learn sequentially while retaining knowledge from the past training rounds. Second, the model has also to deal with concept drift from the distributed data distributions. To address these complexities, we attempt to combine continual and federated learning strategies by proposing a solution inspired by experience replay and generative adversarial concepts for supporting decentralized distributed training. In particular, our approach relies on using limited memory buffers of synthetic privacy-preserving samples and interleaving training on local data and on buffer data. By translating the CL formulation into the task of integrating distributed knowledge with local knowledge, our method enables models to effectively integrate learned representation from local nodes, providing models the capability to generalize across multiple datasets.We test our integrated strategy on two realistic medical image analysis tasks — tuberculosis and melanoma classification — using multiple datasets in order to simulate realistic non-i.i.d. medical data scenarios. Results show that our approach achieves performance comparable to standard (non-federated) learning and significantly outperforms state-of-the-art federated methods in their centralized (thus, more favourable) formulation.},
  note = {https://ieeexplore.ieee.org/document/10350429},
  keywords = {ai},
  pubstate = {published},
  tppubtype = {inproceedings},
  abbr = {ICCVW}
}

@article{23:fl:patterns,
  title = {MERGE: A model for multi-input biomedical federated learning},
  author = {Bruno Casella and Walter Riviera and Marco Aldinucci and Gloria Menegaz},
  url = {https://www.sciencedirect.com/science/article/pii/S2666389923002404},
  doi = {10.1016/j.patter.2023.100856},
  issn = {2666-3899},
  year = {2023},
  date = {2023-01-01},
  journal = {Patterns},
  pages = {100856},
  abstract = {Driven by the deep learning (DL) revolution, artificial intelligence (AI) has become a fundamental tool for many biomedical tasks, including analyzing and classifying diagnostic images. Imaging, however, is not the only source of information. Tabular data, such as personal and genomic data and blood test results, are routinely collected but rarely considered in DL pipelines. Nevertheless, DL requires large datasets that often must be pooled from different institutions, raising non-trivial privacy concerns. Federated learning (FL) is a cooperative learning paradigm that aims to address these issues by moving models instead of data across different institutions. Here, we present a federated multi-input architecture using images and tabular data as a methodology to enhance model performance while preserving data privacy. We evaluated it on two showcases: the prognosis of COVID-19 and patients' stratification in Alzheimer's disease, providing evidence of enhanced accuracy and F1 scores against single-input models and improved generalizability against non-federated models.},
  keywords = {ai, epi, icsc},
  pubstate = {published},
  tppubtype = {article},
  abbr = {Patterns}
}

@article{23:casella:ultra,
  title = {Predictors of target lesion failure after treatment of left main, bifurcation, or chronic total occlusion lesions with ultrathin-strut drug-eluting coronary stents in the ULTRA registry},
  author = {Ovidio Filippo and Francesco Bruno and Tineke H. Pinxterhuis and Mariusz Gasior and Leor Perl and Luca Gaido and Domenico Tuttolomondo and Antonio Greco and Roberto Verardi and Gianluca Lo Martire and Mario Iannaccone and Attilio Leone and Gaetano Liccardo and Serena Caglioni and Rocio González Ferreiro and Giulio Rodinò and Giuseppe Musumeci and Giuseppe Patti and Irene Borzillo and Giuseppe Tarantini and Wojciech Wańha and Bruno Casella and Eline H Ploumen and Lukasz Pyka and Ran Kornowski and Andrea Gagnor and Raffaele Piccolo and Sergio Raposeiras Roubin and Davide Capodanno and Paolo Zocca and Federico Conrotto and Gaetano M De Ferrari and Clemens Birgelen and Fabrizio D'Ascenzo},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/ccd.30696},
  doi = {10.1002/ccd.30696},
  year = {2023},
  date = {2023-01-01},
  journal = {Catheterization and Cardiovascular Interventions},
  abstract = {Background: Data about the long-term performance of new-generation ultrathin-strut drug-eluting stents (DES) in challenging coronary lesions, such as left main (LM), bifurcation, and chronic total occlusion (CTO) lesions are scant. Methods: The international multicenter retrospective observational ULTRA study included consecutive patients treated from September 2016 to August 2021 with ultrathin-strut (<70µm) DES in challenging de novo lesions. Primary endpoint was target lesion failure (TLF): composite of cardiac death, target-lesion revascularization (TLR), target-vessel myocardial infarction (TVMI), or definite stent thrombosis (ST). Secondary endpoints included all-cause death, acute myocardial infarction (AMI), target vessel revascularization, and TLF components. TLF predictors were assessed with Cox multivariable analysis. Results: Of 1801 patients (age: 66.6$±$11.2 years; male: 1410 [78.3%]), 170 (9.4%) experienced TLF during follow-up of 3.1$±$1.4 years. In patients with LM, CTO, and bifurcation lesions, TLF rates were 13.5%, 9.9%, and 8.9%, respectively. Overall, 160 (8.9%) patients died (74 [4.1%] from cardiac causes). AMI and TVMI rates were 6.0% and 3.2%, respectively. ST occurred in 11 (1.1%) patients while 77 (4.3%) underwent TLR. Multivariable analysis identified the following predictors of TLF: age, STEMI with cardiogenic shock, impaired left ventricular ejection fraction, diabetes, and renal dysfunction. Among the procedural variables, total stent length increased TLF risk (HR: 1.01, 95% CI: 1-1.02 per mm increase), while intracoronary imaging reduced the risk substantially (HR: 0.35, 95% CI: 0.12-0.82). Conclusions: Ultrathin-strut DES showed high efficacy and satisfactory safety, even in patients with challenging coronary lesions. Yet, despite using contemporary gold-standard DES, the association persisted between established patient- and procedure-related features of risk and impaired 3-year clinical outcome.},
  keywords = {ai, cardio},
  pubstate = {published},
  tppubtype = {article},
  abbr = {CCI}
}

@inproceedings{22:ml4astro,
  title = {Federated Learning meets HPC and cloud},
  author = {Iacopo Colonnelli and Bruno Casella and Gianluca Mittone and Yasir Arfat and Barbara Cantalupo and Roberto Esposito and Alberto Riccardo Martinelli and Doriana Medić and Marco Aldinucci},
  editor = {Filomena Bufano and Simone Riggi and Eva Sciacca and Francesco Schillirò},
  url = {https://iris.unito.it/retrieve/3ac66baa-9d9a-4e9f-94a5-13700694d8aa/ML4Astro.pdf},
  doi = {10.1007/978-3-031-34167-0_39},
  isbn = {978-3-031-34167-0},
  year = {2023},
  date = {2023-01-01},
  booktitle = {Astrophysics and Space Science Proceedings},
  volume = {60},
  pages = {193–199},
  publisher = {Springer},
  address = {Catania, Italy},
  abstract = {HPC and AI are fated to meet for several reasons. This article will discuss some of them and argue why this will happen through the set of methods and technologies that underpin cloud computing. As a paradigmatic example, we present a new federated learning system that collaboratively trains a deep learning model in different supercomputing centers. The system is based on the StreamFlow workflow manager designed for hybrid cloud-HPC infrastructures.},
  howpublished = {Machine Learning for Astrophysics (ML4ASTRO)},
  note = {Keynote talk},
  keywords = {across, eupilot, streamflow},
  pubstate = {published},
  tppubtype = {inproceedings},
  selected = {true},
  abbr = {ML4ASTRO}
}

@inproceedings{casella2022benchmarking,
  title = {Benchmarking FedAvg and FedCurv for Image Classification Tasks},
  author = {Bruno Casella and Roberto Esposito and Carlo Cavazzoni and Marco Aldinucci},
  editor = {Marco Anisetti and Angela Bonifati and Nicola Bena and Claudio Ardagna and Donato Malerba},
  url = {https://ceur-ws.org/Vol-3340/paper40.pdf},
  year = {2022},
  date = {2022-01-01},
  booktitle = {Proceedings of the 1st Italian Conference on Big Data and Data Science, ITADATA 2022, September 20-21, 2022},
  volume = {3340},
  publisher = {CEUR-WS.org},
  series = {CEUR Workshop Proceedings},
  abstract = {Classic Machine Learning (ML) techniques require training on data available in a single data lake (either centralized or distributed). However, aggregating data from different owners is not always convenient for different reasons, including security, privacy and secrecy. Data carry a value that might vanish when shared with others; the ability to avoid sharing the data enables industrial applications where security and privacy are of paramount importance, making it possible to train global models by implementing only local policies which can be run independently and even on air-gapped data centres. Federated Learning (FL) is a distributed machine learning approach which has emerged as an effective way to address privacy concerns by only sharing local AI models while keeping the data decentralized. Two critical challenges of Federated Learning are managing the heterogeneous systems in the same federated network and dealing with real data, which are often not independently and identically distributed (non-IID) among the clients. In this paper, we focus on the second problem, i.e., the problem of statistical heterogeneity of the data in the same federated network. In this setting, local models might be strayed far from the local optimum of the complete dataset, thus possibly hindering the convergence of the federated model. Several Federated Learning algorithms, such as FedAvg, FedProx and Federated Curvature (FedCurv), aiming at tackling the non-IID setting, have already been proposed. This work provides an empirical assessment of the behaviour of FedAvg and FedCurv in common non-IID scenarios. Results show that the number of epochs per round is an important hyper-parameter that, when tuned appropriately, can lead to significant performance gains while reducing the communication cost. As a side product of this work, we release the non-IID version of the datasets we used so to facilitate further comparisons from the FL community.},
  keywords = {eupilot},
  pubstate = {published},
  tppubtype = {inproceedings}
}

@inproceedings{22:VISAPP:transferlearning,
  title = {Transfer Learning via Test-time Neural Networks Aggregation},
  author = {Bruno Casella and Alessio Chisari and Sebastiano Battiato and Mario Giuffrida},
  editor = {Giovanni Maria Farinella and Petia Radeva and Kadi Bouatouch},
  url = {https://iris.unito.it/retrieve/handle/2318/1844159/947123/TRANSFER_LEARNING_VIA_TEST_TIME_NEURAL_NETWORKS_AGGREGATION.pdf},
  doi = {10.5220/0010907900003124},
  isbn = {978-989-758-555-5},
  year = {2022},
  date = {2022-01-01},
  booktitle = {Proceedings of the 17th International Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications, VISIGRAPP 2022, Volume 5: VISAPP, Online Streaming, February 6-8, 2022},
  pages = {642–649},
  publisher = {SciTePress},
  organization = {INSTICC},
  abstract = {It has been demonstrated that deep neural networks outperform traditional machine learning. However, deep networks lack generalisability, that is, they will not perform as good as in a new (testing) set drawn from a different distribution due to the domain shift. In order to tackle this known issue, several transfer learning approaches have been proposed, where the knowledge of a trained model is transferred into another to improve performance with different data. However, most of these approaches require additional training steps, or they suffer from catastrophic forgetting that occurs when a trained model has overwritten previously learnt knowledge. We address both problems with a novel transfer learning approach that uses network aggregation. We train dataset-specific networks together with an aggregation network in a unified framework. The loss function includes two main components: a task-specific loss (such as cross-entropy) and an aggregation loss. The proposed aggregation loss allows our model to learn how trained deep network parameters can be aggregated with an aggregation operator. We demonstrate that the proposed approach learns model aggregation at test time without any further training step, reducing the burden of transfer learning to a simple arithmetical operation. The proposed approach achieves comparable performance w.r.t. the baseline. Besides, if the aggregation operator has an inverse, we will show that our model also inherently allows for selective forgetting, i.e., the aggregated model can forget one of the datasets it was trained on, retaining information on the others.},
  keywords = {ai},
  pubstate = {published},
  tppubtype = {inproceedings},
  abbr = {VISAPP}
}